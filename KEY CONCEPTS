1. 机器学习（machine learning）：研究如何通过计算的手段，利用“经验”改善系统自身的性能。
  · 机器学习是一门“学科”
  · 机器学习是基于“经验”做出判断

2. “经验”：在计算机中以“数据”形式存在
  · 机器学习研究在计算机上从数据中产生“模型”（model）的算法，即“学习算法”（learning algorithm）
  · 机器学习是研究关于“学习算法”的学问
  · 经机器学习得到学习算法，把经验数据提供它，即可基于这些数据产生模型
  · 在面对新情况（输入新数据），模型会给出我们提供相应的判断
  
3. 模型（model）：泛指从数据中学得的结果
  · 有的文献，“模型”指全局性结果（例如一颗决策树），“模式”指局部性结果（例如一条规则） 
  
4. 数据
  · 数据集（data set）：记录的集合（有时将整个数据集称为一个“样本”，因它可看作对样本空间的一个采样）
  · 示例（instance）/ 样本（sample）：关于一个事件或对象的描述的每条记录
  · 属性（attribute）/ 特征（feature）：反映事件或对象在某方面的表现或性质的事项
  · 属性值（attribute value）：属性上的取值
  · 属性空间（attribute space）/ 样本空间（sample space）/ 输入空间：属性张成的空间。由于空间中的每个点对应一个坐标向量，任意示例亦可称为属性空间内的特征向量（feature vector）

例：一批关于西瓜的数据：
   （色泽=青绿；根蒂=蜷缩；敲声=浊响）、（色泽=乌黑；根蒂=稍蜷；敲声=沉闷）、（色泽=浅白；根蒂=硬挺；敲声=清脆）……每对括号内是一条记录，“=”即“取值为”
   · 这些记录的集合——数据集
   · 每条记录——示例/样本
   · “色泽”、“根蒂”、“敲声”——属性
   · “青绿”、“乌黑”——属性值
   · “色泽”、“根蒂”、“敲声”作为三个坐标值，则它们张成一个用于描述西瓜的三维空间，每个西瓜都可以在这个空间中找到自己的坐标位置
   
 数字化表示：
   · D={x1,x2,……,xm}包含m个示例的数据集
   · xi={xi1;xi2;……;xid}每个示例由d个属性描述,每个示例xi是d维样本空间X中的一个向量,xi∈X，xij是xi的第j个属性上的取值，d是样本xi的“维数”（dimensionality）

5. 学习（learning）/ 训练（training）：从数据中学得模型的过程，该过程是通过执行某个学习算法来完成的
   · 训练数据（training data）：训练过程中使用的数据，其中每个样本称为一个“训练样本”（training sample）
   · 训练集（training set）：训练样本组成的集合
   · 假设（typothesis）：训练学得的模型对应了关于数据的某种潜在的规律
   · 真相/真实（ground-truth）：潜在规律自身
   · 学习器（learn）：学习过程就是为了找出或逼近真相，故模型可称为学习器，可看作学习算法在给定数据和参数空间上的实例化
   注：学习算法通常有参数需要设置，使用不同的参数值和（或）训练数据，将产生不同的结果
   
6. 训练样本的“结果”信息：
例：（（色泽=青绿；根蒂=蜷曲；敲声=浊响），好瓜）
   · 标记（label）：关于示例结果的信息——“好瓜”
   · 样例（example）：拥有标记信息的示例（若将标记看作对象本身的一部分，则“样例”也可称为“样本”）
   · 标记空间（label space）/输出空间：所有标记的集合
     eg：(xi,yi)表示第i个样例，其中yi∈Y是示例xi的标记，Y是所有标记的集合——标记空间/输出空间
   
7. 预测任务：
   · 分类（classification）：预测的是离散值，如“好瓜”、“坏瓜”
   · 回归（regression）：预测的是连续值，如西瓜成熟度0.95、0.37
   · 二分类（binary classification）任务：只涉及两个类别的分类任务，通常其中一个类为“正类”（positive class），另一个为“反类/负类”（negative class）
     eg：对训练集{(x1,y1),(x2,y2),...,(xm,ym)}进行学习以完成预测任务，需建立一个从输入空间X到输出空间K的映射 f：X → Y
   · 二分类任务，通常Y={-1，+1}或{0,1}
   · 多分类（multi-class classification）任务：涉及多个类别
     eg：[续]多分类任务：|Y|>2;回归任务：Y=R，实数集
   
8. 测试（testing）:学得模型后，使用其进行预测的过程
   测试样本（testing sample）：被预测的样本
   eg：学得f后，对测试例x，可得到其预测标记y=f(x)
   
9. 聚类（clustering）：将训练集中的示例分成若干组，每组称为一个“簇”（cluster）
   自动形成的簇可能对应一些潜在的概念划分，这样的学习过程有助于我们了解数据内在的规律，能为更深入地分析数据建立基础。
   学习过程中使用的训练样本是通常不拥有标记信息。
   eg：对西瓜数据集进行聚类学习，可能对应潜在概念划分：“浅色瓜”、“深色瓜”或“本地瓜”、“外地瓜”且这些概念我们事先不知道，即不拥有标记信息
   
10. 学习任务分类：
    根据训练集是否拥有标记信息，学习任务可大致分为两类 —— 监督学习（supervised learning），如回归/无监督学习（unsupervised learning），如聚类

11.机器学习的目标：
   · 泛化（generalization）能力：学得模型适用于新样本的能力。
     · 机器学习的目标是使学得的模型能很好地适用于“新样本”，而不仅是在训练样本上工作得好
     · 具有强泛化的模型能很好地适用于整个样本空间
   · 分布（distribution）D：假设样本空间中全体样本服从一个未知“分布”（distribution）D，我们获得的样本都是独立地从这个分布上采样获得的，即“独立同分布”（independent and identically）
     · 一般，训练样本越多，我们得到的关于D的信息越多，这样就有可能获得具有强泛化能力的模型
 
 12.假设空间
     · 归纳（induction）：从特殊到一般的“泛化”（generalization）过程，即从具体的事实归结出一般性规律
     · 演绎（specialization）：从基础原理推演出具体状况。
       eg:数学公理系统，基于一组公理和推理规则推导出与之相洽的定理————演绎;
          “从样例中学习”————归纳，归纳学习（inductive learning）
     · 归纳学习：有狭义和广义之分
       广义的归纳学习：从样例中学习
       狭义的归纳学习：从训练数据中学得概念（concept）———— 概念学习/概念形成
     · 假设空间：学习过程可看作一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到所有与训练集“匹配”（fit）的假设，即能够将训练集中的样本判断正确的假设。
       假设的表示一旦确认，假设空间及其规模大小就确定了。
     · 假设空间搜索策略：例如自顶向下——从一般到特殊；自底向上——从特殊到一般。
       搜索过程中不断删除与正例不一致的假设，即我们所学得的结果。
     · 版本空间（version space）：现实生活中常需要面临很大的假设空间，但学习过程是基于有限样本训练集进行的，故可能存在多个假设与训练集一致，即存在一个与训练集一致的“假设集合”
     
 13.归纳偏好
     当对于同一训练集学习得到多个与其一致的假设，如何对它们进行选择？取决于学习算法本身的“偏好”
     · 归纳偏好（inductive bias）：机器学习算法在学习过程中对某种类型假设的偏好，可简称为“偏好”
       如果没有“偏好”，学习算法产生的模型每次在进行预测时随机抽选训练集上的等效假设。此时对于一个新样本，学得模型可能会告诉我们不同的结果，这样的学习无意义。
       eg：若认为相似的样本应有相似的输出，则在存在多条曲线与有限样本训练集一致，对应的学习算法可能偏好比较“平滑”的曲线，而不是“崎岖”的曲线。
     · “奥卡姆剃刀”（Occam's razor）：引导算法确立“正确的”的偏好的一般性原则——常用的，最基本的原则“若有多个假设与观察一致，则选择最简单的那个”
     · “没有免费的午餐”定理（No Free Lunch Theorem，NFL）：无论学习算法A多聪明，学习算法B多笨拙，它们的期望性能相同。
       - 前提：所有“问题”出现的机会相同、或所有问题同等重要——假设f的均匀分布，实际情况并非如此。
       - 实际：一般我们只关注自己试图解决的问题（例如某个具体的任务），希望它能找到一个解决方案，至于这个解决方案在别的问题、甚至在相似问题上是否为好方案，我们并不关心
       - 启发：NFL定理指出脱离具体问题，空泛地谈论“什么学习算法更好”无意义，因为若考虑所有潜在的问题，则所有算法都一样好;
               要谈论算法的相对优劣，必须针对具体的学习问题;
               在某些问题上表现好的学习算法，在另一些问题上却可能不尽人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性作用。
       
     
   
   
  

   
